{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Noticias to Embedding\n",
    "\n",
    "Embeddings a utilizar (todos con `lowercase`):\n",
    "\n",
    "- Beto (Bert entrenado al español)\n",
    "- Elmo\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from os.path import join, exists\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Descargar archivos necesarios por nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer, procesar daaset y generar vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que i...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que recopila mangas descontinu...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                                text        frames  \n",
       "0  japón registró un nuevo déficit comercial réco...  [0, 1, 0, 0]  \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...  [1, 0, 0, 1]  \n",
       "2  la misteriosa oferta por esteban paredes que i...  [1, 0, 0, 0]  \n",
       "3  la familia maratón que causó revuelo en holand...  [0, 0, 1, 0]  \n",
       "4  crean sitio web que recopila mangas descontinu...  [0, 1, 0, 0]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('NewsInformation.csv')\n",
    "\n",
    "# Cargar texto asociado a cada fila.\n",
    "def load_text(code):\n",
    "    text_path = join('TXT', \"{}.txt\".format(code))\n",
    "    if not exists(text_path):\n",
    "        return \"None\"\n",
    "    try:\n",
    "        with open(text_path, encoding=\"UTF-8\") as file:\n",
    "            return file.read().replace(\"\\r\", \"\").replace(\"\\n\", \" \")\n",
    "    except:\n",
    "        with open(text_path, encoding=\"latin-1\") as file:\n",
    "            return file.read().replace(\"\\r\", \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "df[\"original_text\"] = df.code.map(load_text)\n",
    "# Filtrar None\n",
    "df = df[df[\"original_text\"] != \"None\"]\n",
    "\n",
    "# Aplicar tokenizador al texto y minuscula\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9áéíóú]+')\n",
    "df['text'] = df['original_text'].apply(lambda text: \" \".join([t for t in tkr.tokenize(text) ]).lower() )\n",
    "\n",
    "# Dejar como lista los frames\n",
    "df['frames'] = [np.array(x) for x in zip(df.conflict_fr.tolist(), df.econ_fr.tolist(), df.humint_fr.tolist(), df.morality_fr.tolist()) ]\n",
    "\n",
    "df = df[['original_text', 'text', 'frames']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14513"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_list = []\n",
    "word_doc_counter = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row.text.split()\n",
    "\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word.isnumeric():\n",
    "            new_text.append(\"[NUM]\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    word_list.extend(new_text)\n",
    "    \n",
    "    for word in set(new_text): # Unique words\n",
    "        if word not in word_doc_counter:\n",
    "            word_doc_counter[word] = 0\n",
    "        word_doc_counter[word] += 1\n",
    "\n",
    "counter = Counter(word_list)\n",
    "\n",
    "K = 5\n",
    "# Solo quedarme con palabras que aparecen en 5 o más documentos.\n",
    "vocabulary = [(word, count) for word, count in counter.items() if word_doc_counter[word] >= K]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plantaciones: 10\r\n",
      "lamentando: 10\r\n",
      "cerrados: 10\r\n",
      "alessandro: 10\r\n",
      "consejero: 10\r\n",
      "tituló: 10\r\n",
      "temen: 10\r\n",
      "cha: 10\r\n",
      "vih: 10\r\n",
      "percató: 10\r\n"
     ]
    }
   ],
   "source": [
    "# Ordenar por su frecuencia de menor a mayor\n",
    "vocabulary.sort(key=lambda x:x[1])\n",
    "vocabulary = vocabulary[-10000:] # OJO: Aquí te faltó invertir la lista de mayor a menor y ahí sacar los 10.000 \n",
    "\n",
    "with open(\"vocabulary_corpus_counter.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for word, count in vocabulary:\n",
    "        file.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "        \n",
    "!head -10 vocabulary_corpus_counter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames  conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]          0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]          1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]          1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]          0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]          0   \n",
       "\n",
       "   economico  humanidad  moral  \n",
       "0          1          0      0  \n",
       "1          0          0      1  \n",
       "2          0          0      0  \n",
       "3          0          1      0  \n",
       "4          1          0      0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_set = set([x[0] for x in vocabulary])\n",
    "\n",
    "preprocess_text = []\n",
    "for i, x in df.iterrows():\n",
    "    text = x.text.split()\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in vocabulary_set:\n",
    "            new_text.append(word)\n",
    "        elif word.isnumeric():\n",
    "            new_text.append(\"[NUM]\")\n",
    "        else:\n",
    "            new_text.append(\"[UNK]\")\n",
    "            \n",
    "    preprocess_text.append(\" \".join(new_text))\n",
    "    \n",
    "df['preprocess_text'] = preprocess_text\n",
    "\n",
    "#creating vocabulary\n",
    "vocab = [x.strip().split(\": \")[0] for x in open(\"vocabulary_corpus_counter.txt\", encoding=\"UTF-8\").readlines()]\n",
    "\n",
    "vocab_to_idx = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "for word in vocab:\n",
    "    vocab_to_idx[word] = len(vocab_to_idx)\n",
    "    \n",
    "def encode_sentence(text, vocab_to_idx):\n",
    "    enc1 = [vocab_to_idx.get(word, 1) for word in text.split()]\n",
    "    return np.array(enc1)\n",
    "\n",
    "df['encoded'] = df['preprocess_text'].apply(lambda x: np.array(encode_sentence(x, vocab_to_idx)))\n",
    "\n",
    "df[\"conflicto\"] = [x[0] for x in df.frames]\n",
    "df[\"economico\"] = [x[1] for x in df.frames]\n",
    "df[\"humanidad\"] = [x[2] for x in df.frames]\n",
    "df[\"moral\"] = [x[3] for x in df.frames]\n",
    "\n",
    "del df[\"text\"]\n",
    "df = df[['original_text', 'preprocess_text', 'encoded', 'frames', 'conflicto', 'economico', 'humanidad', 'moral']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"datasets/preprocess_dataset.npy\", df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "histogram = []\n",
    "for i, x in df.iterrows():\n",
    "    text = x.preprocess_text.split()\n",
    "    count = sum([1 for x in text if x != \"[UNK]\"])\n",
    "    histogram.append(count)\n",
    "        \n",
    "# the histogram of the data\n",
    "_, bins, _ = plt.hist(histogram, range=[0, 1000])\n",
    "plt.title('Texts Length Histogram')\n",
    "plt.xlabel('Nº tokens')\n",
    "plt.ylabel('Number of texts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conflicto    1117\n",
      "economico     550\n",
      "humanidad    1497\n",
      "moral         754\n",
      "dtype: object\n",
      "\n",
      "Percentage\n",
      "\n",
      "conflicto    0.285094\n",
      "economico    0.140378\n",
      "humanidad    0.382083\n",
      "moral        0.192445\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "columns = ['original_text', 'preprocess_text', 'encoded', 'frames',\n",
    "           'conflicto', 'economico', 'humanidad', 'moral']\n",
    "\n",
    "df = np.load(\"datasets/preprocess_dataset.npy\", allow_pickle=True)\n",
    "df = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "labels_df = df[['conflicto', 'economico', 'humanidad', 'moral']]\n",
    "print(labels_df.sum(axis=0))\n",
    "print(\"\\nPercentage\\n\")\n",
    "print(labels_df.sum(axis=0)/labels_df.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     973\n",
      "1    1352\n",
      "2     732\n",
      "3     314\n",
      "4      40\n",
      "Name: frames, dtype: int64\n",
      "\n",
      "Percentage\n",
      "\n",
      "0    0.285254\n",
      "1    0.396365\n",
      "2    0.214600\n",
      "3    0.092055\n",
      "4    0.011727\n",
      "Name: frames, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "count_frames = df.frames.map(lambda x: sum(x))\n",
    "\n",
    "print(count_frames.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nPercentage\\n\")\n",
    "print(count_frames.value_counts().sort_index()/count_frames.value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing aditional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EM, lunes, 1.10.2007 ACUSA FALTA A \"PRINCIPIO ...</td>\n",
       "      <td>[UNK] lunes [NUM] [NUM] [NUM] acusa falta a pr...</td>\n",
       "      <td>[1, 9840, 10000, 10000, 10000, 8486, 9571, 999...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Edil reconoce error por omitir empresas en dec...</td>\n",
       "      <td>edil reconoce error por [UNK] empresas en decl...</td>\n",
       "      <td>[4607, 7840, 9050, 9990, 1, 9455, 9997, 8748, ...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EM, viernes, 2.11.2007 A LA COMISIÓN INVESTIGA...</td>\n",
       "      <td>[UNK] viernes [NUM] [NUM] [NUM] a la [UNK] n i...</td>\n",
       "      <td>[1, 9794, 10000, 10000, 10000, 9995, 9999, 1, ...</td>\n",
       "      <td>[1, 0, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Moneda analiza texto de Lagos por Transanti...</td>\n",
       "      <td>la moneda analiza texto de lagos por transanti...</td>\n",
       "      <td>[9999, 9383, 5236, 8685, 10001, 8798, 9990, 85...</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PM, lunes, 26.3.2007 Moreira: “que Lagos asuma...</td>\n",
       "      <td>pm lunes [NUM] [NUM] [NUM] moreira que lagos [...</td>\n",
       "      <td>[6583, 9840, 10000, 10000, 10000, 6095, 9996, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  EM, lunes, 1.10.2007 ACUSA FALTA A \"PRINCIPIO ...   \n",
       "1  Edil reconoce error por omitir empresas en dec...   \n",
       "2  EM, viernes, 2.11.2007 A LA COMISIÓN INVESTIGA...   \n",
       "3  La Moneda analiza texto de Lagos por Transanti...   \n",
       "4  PM, lunes, 26.3.2007 Moreira: “que Lagos asuma...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  [UNK] lunes [NUM] [NUM] [NUM] acusa falta a pr...   \n",
       "1  edil reconoce error por [UNK] empresas en decl...   \n",
       "2  [UNK] viernes [NUM] [NUM] [NUM] a la [UNK] n i...   \n",
       "3  la moneda analiza texto de lagos por transanti...   \n",
       "4  pm lunes [NUM] [NUM] [NUM] moreira que lagos [...   \n",
       "\n",
       "                                             encoded        frames  conflicto  \\\n",
       "0  [1, 9840, 10000, 10000, 10000, 8486, 9571, 999...  [1, 0, 0, 1]          1   \n",
       "1  [4607, 7840, 9050, 9990, 1, 9455, 9997, 8748, ...  [1, 0, 0, 1]          1   \n",
       "2  [1, 9794, 10000, 10000, 10000, 9995, 9999, 1, ...  [1, 0, 1, 1]          1   \n",
       "3  [9999, 9383, 5236, 8685, 10001, 8798, 9990, 85...  [1, 1, 1, 1]          1   \n",
       "4  [6583, 9840, 10000, 10000, 10000, 6095, 9996, ...  [1, 0, 0, 0]          1   \n",
       "\n",
       "   economico  humanidad  moral  \n",
       "0          0          0      1  \n",
       "1          0          0      1  \n",
       "2          0          1      1  \n",
       "3          1          1      1  \n",
       "4          0          0      0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating vocabulary\n",
    "vocab = [x.strip().split(\": \")[0] for x in open(\"vocabulary_corpus_counter.txt\", encoding=\"UTF-8\").readlines()]\n",
    "\n",
    "vocab_to_idx = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "for word in vocab:\n",
    "    vocab_to_idx[word] = len(vocab_to_idx)\n",
    "    \n",
    "def encode_sentence(text, vocab_to_idx):\n",
    "    enc1 = [vocab_to_idx.get(word, 1) for word in text.split()]\n",
    "    return np.array(enc1)\n",
    "\n",
    "df_2 = pd.read_csv(\"news_dataset_2.tsv\", sep=\"\\t\")\n",
    "df_2[\"original_text\"] = df_2[\"text\"]\n",
    "\n",
    "# Aplicar tokenizador al texto y minuscula\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9áéíóú]+')\n",
    "df_2['text'] = df_2['text'].apply(lambda text: \" \".join([t for t in tkr.tokenize(text) ]).lower() )\n",
    "df_2['frames'] = [np.array(x) for x in zip(df_2.conflict.tolist(), df_2.economy.tolist(), df_2.humanity.tolist(), df_2.moral.tolist()) ]\n",
    "\n",
    "df_2 = df_2[[\"original_text\", \"text\", \"frames\"]]\n",
    "\n",
    "preprocess_text_2 = []\n",
    "for i, x in df_2.iterrows():\n",
    "    text = x.text.split()\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in vocabulary_set:\n",
    "            new_text.append(word)\n",
    "        elif word.isnumeric():\n",
    "            new_text.append(\"[NUM]\")\n",
    "        else:\n",
    "            new_text.append(\"[UNK]\")\n",
    "            \n",
    "    preprocess_text_2.append(\" \".join(new_text))\n",
    "    \n",
    "df_2['preprocess_text'] = preprocess_text_2\n",
    "df_2['encoded'] = df_2['preprocess_text'].apply(lambda x: np.array(encode_sentence(x, vocab_to_idx)))\n",
    "\n",
    "df_2[\"conflicto\"] = [x[0] for x in df_2.frames]\n",
    "df_2[\"economico\"] = [x[1] for x in df_2.frames]\n",
    "df_2[\"humanidad\"] = [x[2] for x in df_2.frames]\n",
    "df_2[\"moral\"] = [x[3] for x in df_2.frames]\n",
    "\n",
    "del df_2[\"text\"]\n",
    "df_2 = df_2[['original_text', 'preprocess_text', 'encoded', 'frames', 'conflicto', 'economico', 'humanidad', 'moral']]\n",
    "\n",
    "np.save(\"datasets/preprocess_dataset_2.npy\", df_2.values)\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datasets y calcular embeddins\n",
    "- FastText\n",
    "- Elmo\n",
    "- Beto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]         1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]         0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]         0   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         1         0     0  \n",
       "1         0         0     1  \n",
       "2         0         0     0  \n",
       "3         0         1     0  \n",
       "4         1         0     0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['original_text', 'preprocess_text', 'encoded', 'frames', 'conflicto', 'economico', 'humanidad', 'moral']\n",
    "\n",
    "df = np.load(\"datasets/preprocess_dataset.npy\", allow_pickle=True)\n",
    "df = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EM, lunes, 1.10.2007 ACUSA FALTA A \"PRINCIPIO ...</td>\n",
       "      <td>[UNK] lunes [NUM] [NUM] [NUM] acusa falta a pr...</td>\n",
       "      <td>[1, 9840, 10000, 10000, 10000, 8486, 9571, 999...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Edil reconoce error por omitir empresas en dec...</td>\n",
       "      <td>edil reconoce error por [UNK] empresas en decl...</td>\n",
       "      <td>[4607, 7840, 9050, 9990, 1, 9455, 9997, 8748, ...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EM, viernes, 2.11.2007 A LA COMISIÓN INVESTIGA...</td>\n",
       "      <td>[UNK] viernes [NUM] [NUM] [NUM] a la [UNK] n i...</td>\n",
       "      <td>[1, 9794, 10000, 10000, 10000, 9995, 9999, 1, ...</td>\n",
       "      <td>[1, 0, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Moneda analiza texto de Lagos por Transanti...</td>\n",
       "      <td>la moneda analiza texto de lagos por transanti...</td>\n",
       "      <td>[9999, 9383, 5236, 8685, 10001, 8798, 9990, 85...</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PM, lunes, 26.3.2007 Moreira: “que Lagos asuma...</td>\n",
       "      <td>pm lunes [NUM] [NUM] [NUM] moreira que lagos [...</td>\n",
       "      <td>[6583, 9840, 10000, 10000, 10000, 6095, 9996, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  EM, lunes, 1.10.2007 ACUSA FALTA A \"PRINCIPIO ...   \n",
       "1  Edil reconoce error por omitir empresas en dec...   \n",
       "2  EM, viernes, 2.11.2007 A LA COMISIÓN INVESTIGA...   \n",
       "3  La Moneda analiza texto de Lagos por Transanti...   \n",
       "4  PM, lunes, 26.3.2007 Moreira: “que Lagos asuma...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  [UNK] lunes [NUM] [NUM] [NUM] acusa falta a pr...   \n",
       "1  edil reconoce error por [UNK] empresas en decl...   \n",
       "2  [UNK] viernes [NUM] [NUM] [NUM] a la [UNK] n i...   \n",
       "3  la moneda analiza texto de lagos por transanti...   \n",
       "4  pm lunes [NUM] [NUM] [NUM] moreira que lagos [...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [1, 9840, 10000, 10000, 10000, 8486, 9571, 999...  [1, 0, 0, 1]         1   \n",
       "1  [4607, 7840, 9050, 9990, 1, 9455, 9997, 8748, ...  [1, 0, 0, 1]         1   \n",
       "2  [1, 9794, 10000, 10000, 10000, 9995, 9999, 1, ...  [1, 0, 1, 1]         1   \n",
       "3  [9999, 9383, 5236, 8685, 10001, 8798, 9990, 85...  [1, 1, 1, 1]         1   \n",
       "4  [6583, 9840, 10000, 10000, 10000, 6095, 9996, ...  [1, 0, 0, 0]         1   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         0         0     1  \n",
       "1         0         0     1  \n",
       "2         0         1     1  \n",
       "3         1         1     1  \n",
       "4         0         0     0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = np.load(\"datasets/preprocess_dataset_2.npy\", allow_pickle=True)\n",
    "df_2 = pd.DataFrame(df_2, columns=columns)\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertForPreTraining\n",
    "from torch.utils.data import DataLoader, TensorDataset   \n",
    "\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "\n",
    "def convert_example_to_feature(example_row):\n",
    "    text, max_seq_length, tokenizer, cls_token, sep_token = example_row\n",
    "    tokens_a = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    special_tokens_count = 2\n",
    "    tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "    tokens = [cls_token] + tokens\n",
    "    segment_ids = [0] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    input_mask = input_mask + ([0] * padding_length)\n",
    "    segment_ids = segment_ids + ([0] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_examples_to_features(texts, max_seq_length, tokenizer, cls_token, sep_token):\n",
    "    examples = [(text, max_seq_length, tokenizer, cls_token, sep_token) for text in texts]\n",
    "    return [convert_example_to_feature(example) for example in examples]\n",
    "\n",
    "\n",
    "\n",
    "class BetoEmbedding:\n",
    "    def __init__(self, take_mean=True):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('vocab.txt', keep_accents=True)\n",
    "        config = BertConfig.from_json_file('bert_config.json')\n",
    "        config.output_hidden_states = True\n",
    "        self.bert = BertForPreTraining.from_pretrained('pytorch_model.bin', config=config)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.take_mean = take_mean\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "        device = torch.device(\"cuda\")\n",
    "        \n",
    "        self.bert.to(device)\n",
    "\n",
    "        CLS_VECTORS = None\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, token_type_ids = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "                        \n",
    "            # outputs[-1] = salidas de las capas ocultas (hidden_iniciales, capa_1, ..., capa_12)\n",
    "            outputs = outputs[-1][-1] # Tomar de las hidden_state, la última capa\n",
    "\n",
    "            if self.take_mean:\n",
    "                output = torch.mean(outputs, 1)\n",
    "            else:\n",
    "                output = outputs[:, 0, :] # Tomar el primer vector (CLS)\n",
    "                \n",
    "            if CLS_VECTORS is None:\n",
    "                CLS_VECTORS = output.detach().cpu().numpy()\n",
    "            else:\n",
    "                CLS_VECTORS = np.append(CLS_VECTORS, output.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return CLS_VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def get_beto_embedding(x):\n",
    "    torch.manual_seed(4444)\n",
    "    np.random.seed(4444)\n",
    "    \n",
    "    x = np.array([\" \".join(text.split(\" \")[:510]) for text in x])\n",
    "    return BetoEmbedding(take_mean=True)(x)\n",
    "\n",
    "beto_embedding_dataset_1 = get_beto_embedding(df.preprocess_text)\n",
    "beto_embedding_dataset_2 = get_beto_embedding(df_2.preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"datasets/beto_embedding_mean_dataset_1.npy\", beto_embedding_dataset_1)\n",
    "np.save(\"datasets/beto_embedding_mean_dataset_2.npy\", beto_embedding_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def get_beto_embedding(x):\n",
    "    torch.manual_seed(4444)\n",
    "    np.random.seed(4444)\n",
    "    \n",
    "    x = np.array([\" \".join(text.split(\" \")[:510]) for text in x])\n",
    "    return BetoEmbedding(take_mean=False)(x)\n",
    "\n",
    "\n",
    "beto_embedding_dataset_1 = get_beto_embedding(df.preprocess_text)\n",
    "beto_embedding_dataset_2 = get_beto_embedding(df_2.preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"datasets/beto_embedding_cls_dataset_1.npy\", beto_embedding_dataset_1)\n",
    "np.save(\"datasets/beto_embedding_cls_dataset_2.npy\", beto_embedding_dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from ELMoForManyLangs.elmoformanylangs import Embedder\n",
    "\n",
    "elmo = Embedder('ElMoSpanishEmbedding/')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = torch.cuda.is_available\n",
    "\n",
    "torch.cuda.is_available = lambda :False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3410/3411\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Desabilitamos el logging para que la librería de ELMo no haga prints\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "def save_elmo(df, filename):\n",
    "    max_length_text = 512\n",
    "\n",
    "    total = len(df)\n",
    "    elmo_embedding = []\n",
    "    for i, text in enumerate(df.preprocess_text):\n",
    "        print(f\"\\r{i}/{total}\", end=\"\")\n",
    "\n",
    "        embedding = elmo.sents2elmo([text.split()[:max_length_text]], output_layer=-1)[0]\n",
    "        elmo_embedding.append(np.mean(embedding, axis=0))\n",
    "\n",
    "    data_elmo =  np.array(elmo_embedding)\n",
    "\n",
    "    # Salvar Elmo Embeddings comprimodo\n",
    "    np.savez_compressed(f'datasets/{filename}.npz', data_elmo)\n",
    "    print(\"\")\n",
    "    \n",
    "save_elmo(df, \"elmo_dataset_1\")\n",
    "save_elmo(df_2, \"elmo_dataset_2\")\n",
    "\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-09 12:00:56,760 INFO: loading projection weights from embeddings-l-model.vec\n",
      "2021-06-09 12:04:42,330 INFO: loaded (1313423, 300) matrix from embeddings-l-model.vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "wordvectors_file_vec = 'embeddings-l-model.vec'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hfvaldivieso/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/702\n"
     ]
    }
   ],
   "source": [
    "# Generamos diccionario de fastText\n",
    "fastText = {}\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = set(stopwords.words('spanish') + [\"[UNK]\", \"[NUM]\"] )\n",
    "\n",
    "\n",
    "def apply_FastText(text):\n",
    "    data = []\n",
    "    for i, word in enumerate(text):\n",
    "        \n",
    "        if word in spanish_stopwords:\n",
    "            continue\n",
    "            \n",
    "        if word in wordvectors:\n",
    "            data.append(wordvectors[word])\n",
    "            \n",
    "    return np.mean(np.array(data), axis=0)\n",
    "\n",
    "\n",
    "def save_fastText(texts, filename):\n",
    "    total = len(texts)\n",
    "    fastText_data = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\r{i}/{total}\", end=\"\")\n",
    "        embedding = apply_FastText(text.split())\n",
    "        fastText_data.append(embedding)\n",
    "\n",
    "    fastText_data =  np.array(fastText_data)\n",
    "\n",
    "    # Salvar Falstext Embeddings comprimodo\n",
    "    np.savez_compressed(f'datasets/{filename}.npz', fastText_data)\n",
    "    print()\n",
    "\n",
    "save_fastText(df.preprocess_text, \"fasttext_dataset_1\")\n",
    "save_fastText(df_2.preprocess_text, \"fasttext_dataset_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
