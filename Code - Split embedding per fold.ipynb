{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from auxiliar_functions import load_dataset, load_embedding\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import torch \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertForPreTraining\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from AsymmetricLoss import AsymmetricLossOptimized\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import gc\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish') + [\"UNK\"]\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = ['conflicto', 'economico', 'humanidad', 'moral']\n",
    "\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "\n",
    "def convert_example_to_feature(example_row):\n",
    "    text, max_seq_length, tokenizer, cls_token, sep_token = example_row\n",
    "    tokens_a = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    special_tokens_count = 2\n",
    "    tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "    tokens = [cls_token] + tokens\n",
    "    segment_ids = [0] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    input_mask = input_mask + ([0] * padding_length)\n",
    "    segment_ids = segment_ids + ([0] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_examples_to_features(texts, max_seq_length, tokenizer, cls_token, sep_token):\n",
    "    examples = [(text, max_seq_length, tokenizer, cls_token, sep_token) for text in texts]\n",
    "    return [convert_example_to_feature(example) for example in examples]\n",
    "\n",
    "\n",
    "class BetoEmbedding(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size=8, take_mean=True,\n",
    "                 loss_function=\"cross-entropy\", n_epochs=30):\n",
    "        super().__init__()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('vocab.txt', keep_accents=True)\n",
    "        config = BertConfig.from_json_file('bert_config.json')\n",
    "        config.output_hidden_states = True\n",
    "        self.bert = BertForPreTraining.from_pretrained('pytorch_model.bin', config=config)\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name.startswith(\"bert.encoder.layer.11\"):\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.take_mean = take_mean\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.loss_object = AsymmetricLossOptimized()\n",
    "        if loss_function == \"cross-entropy\":\n",
    "            self.loss_object = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(768, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.embedding_dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 4) # Cambiar \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def get_embedding(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        # outputs[-1] = salidas de las capas ocultas (hidden_iniciales, capa_1, ..., capa_12)\n",
    "        outputs = outputs[-1][-1] # Tomar de las hidden_state, la última capa\n",
    "        if self.take_mean:\n",
    "            output = torch.mean(outputs, 1)\n",
    "        else:\n",
    "            output = outputs[:, 0, :] # Tomar el primer vector (CLS)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.get_embedding(input_ids, attention_mask, token_type_ids)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "  \n",
    "\n",
    "    def get_proba(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.sigmoid(self(input_ids, attention_mask, token_type_ids))\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, writer=None, fold_index=None):\n",
    "        model = self.cuda(1)\n",
    "        self.bert.cuda(1)\n",
    "        tokenizer = self.tokenizer\n",
    "        \n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, torch.Tensor(Y))\n",
    "        train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        dataloaders = {\"train\": train_dl}\n",
    "        \n",
    "        if X_val is not None:\n",
    "            features = convert_examples_to_features(X_val, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "            val_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, torch.Tensor(Y_val))\n",
    "            dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        optimizer = AdamW(model.parameters(), lr=0.001)  \n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step in dataloaders:\n",
    "                if step == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                total_muestras = 0.0\n",
    "                total_correctas = 0.0\n",
    "                total_loss = 0\n",
    "                loss_class = [0, 0, 0, 0]\n",
    "                all_logits = None\n",
    "                all_target = None\n",
    "\n",
    "                for batch in dataloaders[step]:\n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    input_ids, attention_mask, token_type_ids, target = [x.cuda(1) for x in batch]\n",
    "                    target = target.cuda(1)                   \n",
    "                    logits = model(input_ids, attention_mask, token_type_ids)          \n",
    "\n",
    "                    loss = self.loss_object(logits, target)\n",
    "                    total_loss += (loss * (target.shape[0] * target.shape[1])).item()\n",
    "\n",
    "                    for i in range(target.shape[1]):\n",
    "                        loss_c = self.loss_object(logits[:, i], target[:, i])\n",
    "                        loss_class[i] += (loss_c * target.shape[0]).item()\n",
    "\n",
    "                    preds = logits >= 0.0\n",
    "                    if all_logits is None:\n",
    "                        all_logits = preds.detach().cpu().numpy()\n",
    "                        all_target = target.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        all_logits = np.append(all_logits, preds.detach().cpu().numpy(), axis=0)\n",
    "                        all_target = np.append(all_target, target.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    total_muestras += (target.shape[0]*target.shape[1])       # Sumamos el tamaño del batch\n",
    "                    \n",
    "                    if step == \"train\":\n",
    "                        loss.backward()                             # Backpropagation\n",
    "                        optimizer.step()                            # Actualizamos parámetros\n",
    "                        correctas = (preds == target).sum().item()  # Acumulamos las correctas durante la época\n",
    "                        total_correctas += correctas               \n",
    "                        accuracy = total_correctas/total_muestras \n",
    "                        \n",
    "                        print(\"\\rEpoca {}: Loss: {:.4f} Accuracy: {:.2f}%\".format(epoch, loss, 100*accuracy),\n",
    "                              end=\"\")\n",
    "\n",
    "                save_data(writer, all_logits, all_target, total_loss, loss_class,\n",
    "                          total_muestras, fold_index, epoch, step)\n",
    "\n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        model = self.cuda(1)\n",
    "        self.bert.cuda(1)\n",
    "        model.eval()\n",
    "        self.bert.eval()\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "        train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "        \n",
    "        all_probs = None\n",
    "        for batch in train_dl:\n",
    "            input_ids, attention_mask, token_type_ids = [x.cuda(1) for x in batch]\n",
    "            probs = model.get_proba(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if all_probs is None:\n",
    "                all_probs = probs.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_probs = np.append(all_probs, probs.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_probs \n",
    "    \n",
    "    def get_all_embeddings(self, X):\n",
    "        model = self.cuda(1)\n",
    "        self.bert.cuda(1)\n",
    "        model.eval()\n",
    "        self.bert.eval()\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "        train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "        \n",
    "        all_embedding = None\n",
    "        for batch in train_dl:\n",
    "            input_ids, attention_mask, token_type_ids = [x.cuda(1) for x in batch]\n",
    "            embeddings = model.get_embedding(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if all_embedding is None:\n",
    "                all_embedding = embeddings.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_embedding = np.append(all_embedding, embeddings.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = ['conflicto', 'economico', 'humanidad', 'moral']\n",
    "vocab = [x.strip().split(\": \")[0] for x in open(\"vocabulary_corpus_counter.txt\", encoding=\"UTF-8\").readlines()]\n",
    "vocab_to_idx = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "WORDS = [\"[PAD]\", \"[UNK]\"]\n",
    "for word in vocab:\n",
    "    vocab_to_idx[word] = len(WORDS)\n",
    "    WORDS.append(word)\n",
    "\n",
    "\n",
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        self.texts = [torch.LongTensor(x) for x in X]\n",
    "        self.len = len(X)\n",
    "        if Y is not None:\n",
    "            self.frames = Y\n",
    "        else:\n",
    "            self.frames = [np.array([0]*4) for _ in range(self.len)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        x = self.texts[item]\n",
    "        y = self.frames[item]\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "def my_collate(data_list):\n",
    "    data_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    X_seq, Y = zip(*data_list)\n",
    "    lengths = [len(x) for x in X_seq]\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X_seq, batch_first=True)\n",
    "    return ((X, lengths), torch.Tensor(Y))\n",
    "\n",
    "\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dim_lstm=50, hidden_size=50,\n",
    "                 n_epochs=30, process_output=\"max\", batch_size=1, \n",
    "                 loss_function=\"cross-entropy\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.process_output = process_output\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_object = AsymmetricLossOptimized()\n",
    "        if loss_function == \"cross-entropy\":\n",
    "            self.loss_object = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "        self.embedding = torch.nn.Embedding(len(WORDS), embedding_size, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size, hidden_dim_lstm, \n",
    "                                  num_layers=2, bidirectional=True,\n",
    "                                  batch_first=True)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(hidden_dim_lstm*2, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()    \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.get_embedding(x)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        x, lengths = x\n",
    "        output = self.embedding(x)\n",
    "        \n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths, batch_first=True)\n",
    "        lstm_out, (hidden, _) = self.lstm(output) # shape = (batch_size, seq_len, hidden_dim*2)\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0)\n",
    "        output = lstm_out\n",
    "        \n",
    "        if self.process_output == \"max\":    \n",
    "            output, _ = torch.max(output, 1) # shape = (batch_size, hidden_dim*2)\n",
    "        elif self.process_output == \"mean\":\n",
    "            mask = output != 0.0\n",
    "            output = (output*mask).sum(dim=1)/mask.sum(dim=1)\n",
    "        else: # last_state       # layers # direction (2) # batch size, # hidden_dim\n",
    "            hidden = hidden.view(2, 2, len(lengths), self.hidden_dim_lstm)[-1]\n",
    "            output = torch.cat([hidden[0], hidden[1]], 1)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_proba(self, x):\n",
    "        return self.sigmoid(self(x))\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, writer=None, fold_index=None):\n",
    "        model = self.cuda(1)\n",
    "        \n",
    "        ds_train = MyTextDataset(X, Y)\n",
    "        train_dl = DataLoader(ds_train, batch_size=self.batch_size, shuffle=True, collate_fn=my_collate)\n",
    "        dataloaders = {\"train\": train_dl}\n",
    "        \n",
    "        if X_val is not None:\n",
    "            ds_val = MyTextDataset(X_val, Y_val)    \n",
    "            dataloaders[\"val\"] = DataLoader(ds_val, batch_size=self.batch_size, collate_fn=my_collate)\n",
    "        \n",
    "        loss_object = self.loss_object\n",
    "        optimizer = AdamW(model.parameters(), lr=0.001)  \n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step in dataloaders:\n",
    "                if step == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                total_muestras = 0.0\n",
    "                total_correctas = 0.0\n",
    "                total_loss = 0\n",
    "                loss_class = [0, 0, 0, 0]\n",
    "                all_logits = None\n",
    "                all_target = None\n",
    "\n",
    "                for (x, length), target in dataloaders[step]:\n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    x = x.cuda(1)\n",
    "                    target = target.cuda(1)                   \n",
    "                    logits = model((x, length))          \n",
    "\n",
    "                    loss = loss_object(logits, target)\n",
    "                    total_loss += loss * (target.shape[0] * target.shape[1])\n",
    "\n",
    "                    for i in range(target.shape[1]):\n",
    "                        loss_c = loss_object(logits[:, i], target[:, i])\n",
    "                        loss_class[i] += loss_c * target.shape[0]\n",
    "\n",
    "                    preds = logits >= 0.0\n",
    "                    if all_logits is None:\n",
    "                        all_logits = preds.detach().cpu().numpy()\n",
    "                        all_target = target.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        all_logits = np.append(all_logits, preds.detach().cpu().numpy(), axis=0)\n",
    "                        all_target = np.append(all_target, target.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    total_muestras += (target.shape[0]*target.shape[1])\n",
    "                    \n",
    "                    if step == \"train\":\n",
    "                        loss.backward()                             \n",
    "                        optimizer.step()                            \n",
    "                        correctas = (preds == target).sum().item() \n",
    "                        total_correctas += correctas               \n",
    "                        accuracy = total_correctas/total_muestras \n",
    "                        \n",
    "                        print(\"\\rEpoca {}: Loss: {:.4f} Accuracy: {:.2f}%\".format(epoch, loss, 100*accuracy),\n",
    "                              end=\"\")\n",
    "\n",
    "                save_data(writer, all_logits, all_target, total_loss, loss_class,\n",
    "                          total_muestras, fold_index, epoch, step)\n",
    "                 \n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        model = self.cuda(1)\n",
    "        model.eval()\n",
    "        \n",
    "        ds_test = MyTextDataset(X)    \n",
    "        test_dl = DataLoader(ds_test, batch_size=1, collate_fn=my_collate)\n",
    "        all_probs = None\n",
    "        for (x, length), _ in test_dl:\n",
    "            x = x.cuda(1)                \n",
    "            probs = self.get_proba((x, length))\n",
    "\n",
    "            if all_probs is None:\n",
    "                all_probs = probs.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_probs = np.append(all_probs, probs.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_probs \n",
    "    \n",
    "    def get_all_embeddings(self, X):\n",
    "        model = self.cuda(1)\n",
    "        model.eval()\n",
    "        \n",
    "        ds_test = MyTextDataset(X)    \n",
    "        test_dl = DataLoader(ds_test, batch_size=1, collate_fn=my_collate)\n",
    "        all_embedding = None\n",
    "        for (x, length), _ in test_dl:\n",
    "            x = x.cuda(1)                \n",
    "            embeddings = self.get_embedding((x, length))\n",
    "\n",
    "            if all_embedding is None:\n",
    "                all_embedding = embeddings.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_embedding = np.append(all_embedding, embeddings.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]         1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]         0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]         0   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         1         0     0  \n",
       "1         0         0     1  \n",
       "2         0         0     0  \n",
       "3         0         1     0  \n",
       "4         1         0     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset(\"preprocess_dataset.npy\")\n",
    "Y_true = np.array([np.array(x) for x in df.frames])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4444)\n",
    "k_fold = IterativeStratification(n_splits=10, order=1)\n",
    "encoded = np.array([np.array(x) for x in df.encoded.values], dtype=object)\n",
    "others_embeddings = {}\n",
    "\n",
    "for mode in [\"fasttext\", \"elmo\", \"beto_embedding_mean\", \"beto_embedding_cls\"]:\n",
    "    others_embeddings[mode] = load_embedding(mode, \"dataset_1\")\n",
    "    \n",
    "models = [\n",
    "    [\"Bi-LSTM\", BiLSTM, {'embedding_size': 200, 'hidden_size': 200, 'hidden_dim_lstm': 300, 'n_epochs': 6, 'process_output': 'max', \"batch_size\": 32, \"loss_function\": \"cross-entropy\"}],\n",
    "    [\"Bi-LSTM_AsymetricLoss\", BiLSTM, {'embedding_size': 200, 'hidden_size': 200, 'hidden_dim_lstm': 300, 'n_epochs': 6, 'process_output': 'max', \"batch_size\": 32, \"loss_function\": \"AsymetricLoss\"}],\n",
    "    [\"Beto-finetunning_cross_entropy\", BetoEmbedding, {'hidden_size': 150, \"batch_size\": 32, 'take_mean': True, 'n_epochs': 4, 'loss_function': \"cross-entropy\"}],\n",
    "    [\"Beto-finetunning_asymetric\", BetoEmbedding, {'hidden_size': 150, \"batch_size\": 32, 'take_mean': True, 'n_epochs': 4, 'loss_function': \"asymetric\"}]\n",
    "]\n",
    "\n",
    "for fold_index, (train, test) in enumerate(k_fold.split(df.preprocess_text.values, Y_true)):\n",
    "    X_train_fold, X_test_fold = df.loc[train], df.loc[test]\n",
    "    encoded_train_fold, encoded_test_fold = encoded[train], encoded[test]\n",
    "    # Another embeddings\n",
    "    for mode in [\"fasttext\", \"elmo\", \"beto_embedding_mean\", \"beto_embedding_cls\"]:\n",
    "        X_train_fold[mode] = list(others_embeddings[mode][train])\n",
    "        X_test_fold[mode] = list(others_embeddings[mode][test])\n",
    "        \n",
    "    # End-to-end embeddings\n",
    "    for name, model_object, args in models:\n",
    "        print(name, fold_index)\n",
    "        model = model_object(**args)\n",
    "        model.load_state_dict(torch.load(f'Models/Fold_{fold_index+1}_{name}.model'))\n",
    "        if name[:4] == \"Beto\":\n",
    "            all_embeddings_train = model.get_all_embeddings(X_train_fold.preprocess_text.values)\n",
    "            all_embeddings_test = model.get_all_embeddings(X_test_fold.preprocess_text.values)\n",
    "        else:\n",
    "            all_embeddings_train = model.get_all_embeddings(encoded_train_fold)\n",
    "            all_embeddings_test = model.get_all_embeddings(encoded_test_fold)\n",
    "        X_train_fold[name] = list(all_embeddings_train)\n",
    "        X_test_fold[name] = list(all_embeddings_test)\n",
    "        \n",
    "    # TF - IDF\n",
    "    tf_idf_pipeline = Pipeline([\n",
    "          ('vect', CountVectorizer(lowercase=False, stop_words = spanish_stopwords)),\n",
    "          ('tfidf', TfidfTransformer(use_idf = True))])\n",
    "    tf_idf_pipeline.fit(X_train_fold.preprocess_text.values) \n",
    "    X_train_fold[\"tf-idf\"] = list(tf_idf_pipeline.transform(X_train_fold.preprocess_text.values).toarray())\n",
    "    X_test_fold[\"tf-idf\"] = list(tf_idf_pipeline.transform(X_test_fold.preprocess_text.values).toarray())\n",
    "    \n",
    "    np.save(f\"datasets/fold_{fold_index+1}_train.npy\", X_train_fold.values)\n",
    "    np.save(f\"datasets/fold_{fold_index+1}_test.npy\", X_test_fold.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "      <th>fasttext</th>\n",
       "      <th>elmo</th>\n",
       "      <th>beto_embedding_mean</th>\n",
       "      <th>beto_embedding_cls</th>\n",
       "      <th>Bi-LSTM</th>\n",
       "      <th>Bi-LSTM_AsymetricLoss</th>\n",
       "      <th>Beto-finetunning_cross_entropy</th>\n",
       "      <th>Beto-finetunning_asymetric</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.10552764, -0.27450845, -0.04605328, -0.244...</td>\n",
       "      <td>[0.0596153, -0.49882528, -0.41697934, 0.367517...</td>\n",
       "      <td>[-0.23091996, -0.10888031, -0.41678736, 0.6554...</td>\n",
       "      <td>[0.36867273, -0.11450332, -0.70039237, 1.38560...</td>\n",
       "      <td>[0.9971235, 0.55105656, 0.03968364, 0.75402415...</td>\n",
       "      <td>[0.33348396, 0.020951403, 0.13927792, -0.02246...</td>\n",
       "      <td>[0.4382636, 0.2400094, 0.67646587, -0.12168871...</td>\n",
       "      <td>[0.2481719, -0.07553123, 0.15339072, -0.693724...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.11113898, -0.27680284, -0.06768549, -0.185...</td>\n",
       "      <td>[0.11967598, -0.7192024, -0.3501902, 0.4666095...</td>\n",
       "      <td>[-0.19575264, -0.063836716, -0.21124944, 0.567...</td>\n",
       "      <td>[0.78442883, -0.46931738, 0.7389578, 1.1557943...</td>\n",
       "      <td>[0.1252701, 0.48678556, -0.0014662797, 0.44017...</td>\n",
       "      <td>[0.08652939, 0.13094953, 0.26094568, 0.0071285...</td>\n",
       "      <td>[0.29874167, -0.14668378, -0.5163954, 0.054965...</td>\n",
       "      <td>[0.17326279, -0.07325241, 0.17947382, 0.368167...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "\n",
       "  economico humanidad moral  \\\n",
       "0         1         0     0   \n",
       "2         0         0     0   \n",
       "\n",
       "                                            fasttext  \\\n",
       "0  [-0.10552764, -0.27450845, -0.04605328, -0.244...   \n",
       "2  [-0.11113898, -0.27680284, -0.06768549, -0.185...   \n",
       "\n",
       "                                                elmo  \\\n",
       "0  [0.0596153, -0.49882528, -0.41697934, 0.367517...   \n",
       "2  [0.11967598, -0.7192024, -0.3501902, 0.4666095...   \n",
       "\n",
       "                                 beto_embedding_mean  \\\n",
       "0  [-0.23091996, -0.10888031, -0.41678736, 0.6554...   \n",
       "2  [-0.19575264, -0.063836716, -0.21124944, 0.567...   \n",
       "\n",
       "                                  beto_embedding_cls  \\\n",
       "0  [0.36867273, -0.11450332, -0.70039237, 1.38560...   \n",
       "2  [0.78442883, -0.46931738, 0.7389578, 1.1557943...   \n",
       "\n",
       "                                             Bi-LSTM  \\\n",
       "0  [0.9971235, 0.55105656, 0.03968364, 0.75402415...   \n",
       "2  [0.1252701, 0.48678556, -0.0014662797, 0.44017...   \n",
       "\n",
       "                               Bi-LSTM_AsymetricLoss  \\\n",
       "0  [0.33348396, 0.020951403, 0.13927792, -0.02246...   \n",
       "2  [0.08652939, 0.13094953, 0.26094568, 0.0071285...   \n",
       "\n",
       "                      Beto-finetunning_cross_entropy  \\\n",
       "0  [0.4382636, 0.2400094, 0.67646587, -0.12168871...   \n",
       "2  [0.29874167, -0.14668378, -0.5163954, 0.054965...   \n",
       "\n",
       "                          Beto-finetunning_asymetric  \\\n",
       "0  [0.2481719, -0.07553123, 0.15339072, -0.693724...   \n",
       "2  [0.17326279, -0.07325241, 0.17947382, 0.368167...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fold.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(\n",
    "                os.path.join(root, file),\n",
    "                os.path.relpath(os.path.join(root, file), path),\n",
    "            )\n",
    "            \n",
    "path = \"Models\"\n",
    "zipf = zipfile.ZipFile(f\"{path}.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "zipdir(path, zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets\"\n",
    "zipf = zipfile.ZipFile(f\"{path}.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "zipdir(path, zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
